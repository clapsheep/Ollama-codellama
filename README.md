# 로컬 LLM을 활용한 코드 분석 도구

이 프로젝트는 로컬에서 실행되는 Large Language Model(LLM)을 활용하여 코드 분석 도구를 개발한 과정과 경험을 담고 있습니다. Ollama를 통해 CodeLlama 모델을 로컬 환경에 구축하고, 테스트 코드 생성과 코드 리뷰 시스템을 구현했습니다.

## 🚀 프로젝트 소개

개발자의 생산성을 향상시키기 위해 로컬 LLM을 활용한 도구를 만들었습니다:

- **테스트 코드 생성기**: TypeScript 소스 코드를 분석하여 Vitest 테스트 코드 자동 생성
- **코드 리뷰어**: 소스 코드를 분석하여 품질, 보안, 성능, 유지보수성 관점에서 리뷰

로컬에서 실행되는 LLM을 활용함으로써 코드의 보안을 유지하면서도 AI의 분석 능력을 활용할 수 있습니다.

## 💻 설치 및 사용법

### 사전 요구사항

- Python 3.8 이상
- [Ollama](https://ollama.com/) 설치
- CodeLlama 모델 (7B 또는 13B)

### 간단 설치 가이드

```bash
# 1. 저장소 클론
git clone https://github.com/yourusername/local-llm-code-analyzer.git
cd local-llm-code-analyzer

# 2. 필요한 라이브러리 설치
pip install requests

# 3. Ollama 모델 다운로드
ollama pull codellama:7b  # 또는 codellama:13b
```

### 사용법

#### 코드 리뷰어 실행

```bash
# Ollama 서버 실행 (별도 터미널에서)
ollama serve

# 코드 리뷰 실행
python code_reviewer.py 경로/대상파일.ts
```

리뷰 결과는 `경로/대상파일.ts.review.md` 파일로 저장됩니다.

#### 테스트 코드 생성기 실행

```bash
python code_generator.py 경로/대상파일.ts
```

생성된 테스트 코드는 `경로/__tests__/대상파일.test.ts` 파일로 저장됩니다.

## 🔍 개발 과정

### 테스트 코드 생성에서 코드 리뷰로

처음에는 테스트 코드 자동 생성에 초점을 맞췄지만, 몇 가지 도전과제를 만났습니다:

- 모델이 복잡한 테스트 케이스를 완벽하게 생성하는 데 한계가 있었습니다
- 모든 엣지 케이스를 다루기 위해 프롬프트 최적화에 많은 시간이 필요했습니다
- 테스트 코드 생성보다 코드 분석에서 더 신뢰할만한 결과를 얻을 수 있었습니다

이러한 경험을 바탕으로, 코드 리뷰 시스템으로 방향을 전환했고 더 나은 결과를 얻을 수 있었습니다. 구조화된 JSON 형식의 분석 결과를 요청하고, 이를 가독성 높은 마크다운 보고서로 변환하는 방식을 채택했습니다.

## 🔄 모델 비교

### CodeLlama 7B vs 13B

두 모델을 테스트한 결과:

| 모델 | 처리 시간 | 특징 |
|------|-----------|------|
| 7B | 64.5초 | 더 빠른 응답, 지시사항 준수 잘함 |
| 13B | 89.8초 | 더 상세한 분석, 때로는 지시를 벗어남 |

더 큰 모델이 항상 더 좋은 결과를 제공하지는 않았으며, 7B 모델이 약 28% 더 빠른 응답 시간을 보였습니다.

### 최적 파라미터 설정

최상의 결과를 위해 다음 파라미터를 사용했습니다:

- **Temperature**: 0.2 (낮게) → 일관된 결과
- **Top_p**: 0.8 (중간값) → 적절한 다양성
- **Max_tokens**: 2000 → 충분한 응답 길이

## 📝 리뷰 보고서 예시

코드 리뷰어는 다음과 같은 형식의 마크다운 보고서를 생성합니다:

```markdown
# 코드 리뷰 보고서: 파일명.ts

## 요약
소스 코드에 대한 전반적인 평가와 주요 특징에 대한 요약

## 발견된 문제점

### 🔴 보안 (심각)
- **설명**: 발견된 보안 취약점 설명
- **제안**: 해결 방법
- **해당 줄**: 123, 124

### 🟡 성능 (중요)
- **설명**: 성능 관련 이슈
- **제안**: 최적화 방법

## 발견된 모범 사례
- ✅ 잘 작성된 오류 처리 로직
- ✅ 명확한 변수 이름과 주석

## 개선 제안
- 💡 함수 분리를 통한 가독성 향상
- 💡 타입 안전성 강화
```

## 🧠 배운 점

1. **로컬 LLM의 가능성과 한계**: 
   - 코드 분석에 매우 유용하지만, 테스트 코드 생성과 같은 정밀한 작업에서는 한계가 있음
   - 적절한 프롬프트 설계가 결과 품질에 큰 영향을 미침

2. **모델 크기와 성능의 관계**: 
   - 더 큰 모델이 항상 더 좋은 결과를 제공하지는 않음
   - 작업 특성에 맞는 모델 선택이 중요

3. **프롬프트 엔지니어링의 중요성**: 
   - 명확하고 구체적인 지시가 필요
   - 구조화된 출력 형식(JSON 등) 요청이 후처리에 유리

4. **오류 처리의 필요성**: 
   - LLM은 항상 예상대로 동작하지 않을 수 있음
   - 견고한 오류 처리 로직이 필수적

## 🚀 향후 개선 방향

1. **테스트 코드 생성 고도화**: 
   - RAG(Retrieval-Augmented Generation) 활용
   - 특정 테스트 라이브러리에 특화된 모델 개발

2. **코드 리뷰 기능 확장**: 
   - 코드 품질 추적 기능
   - 팀 코딩 표준에 맞춘 커스텀 규칙

3. **UI 개발**: 
   - 웹 인터페이스 구축
   - 리뷰 결과 시각화

4. **성능 최적화**: 
   - 비동기 처리 도입
   - 모델 양자화를 통한 효율화

---

이 프로젝트는 로컬 LLM의 실용적인 활용 가능성을 탐색하는 시도로, 개발 생산성 향상을 위한 도구 개발에 관심 있는 분들에게 참고가 되기를 바랍니다.
